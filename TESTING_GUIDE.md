# Testing Guide - Climate Embeddings Project

Komplexn√Ω n√°vod na testovanie v≈°etk√Ωch funkcional√≠t na externom serveri.

## üöÄ R√Ωchly ≈°tart

### 1. Spustenie slu≈æieb

```bash
# Spusti≈• v≈°etky Docker slu≈æby
make docker-compose-up

# Alebo manu√°lne
docker-compose up -d

# Skontrolova≈• stav
docker-compose ps
```

### 2. Verifik√°cia slu≈æieb

```bash
# Skontrolova≈• zdravie v≈°etk√Ωch slu≈æieb
make verify-services

# Alebo jednotlivo
make check-qdrant    # http://localhost:6333
make check-ollama    # http://localhost:11434
make api-health      # http://localhost:8000
```

## üìã Testovanie form√°tov d√°t

### Automatick√Ω test v≈°etk√Ωch form√°tov

```bash
# Spust√≠ kompletn√Ω test suite pre v≈°etky form√°ty
make test-formats
```

Testuje:
- ‚úÖ **NetCDF** (.nc, .nc4) - klimatick√© modely
- ‚úÖ **GeoTIFF** (.tif, .tiff) - rastrov√© mapy
- ‚úÖ **CSV** (.csv) - tabulkov√© d√°ta
- ‚úÖ **GRIB** (.grib, .grb2) - meteorologick√© d√°ta
- ‚úÖ **HDF5** (.h5, .hdf5) - vedeck√© d√°ta
- ‚úÖ **ASCII Grid** (.asc) - grid d√°ta
- ‚úÖ **Zarr** (.zarr) - chunked arrays
- ‚úÖ **ZIP** (.zip) - arch√≠vy s viacer√Ωmi s√∫bormi

### Manu√°lne testovanie jednotliv√Ωch form√°tov

#### NetCDF
```bash
docker-compose exec web-api python << 'EOF'
from climate_embeddings.loaders import load_raster_auto

result = load_raster_auto("data/external/era5_temperature.nc")
print(f"‚úì Loaded {len(result.embeddings)} embeddings")
print(f"  Shape: {result.embeddings.shape}")
print(f"  Variables: {result.metadata.get('variables', [])}")
EOF
```

#### GeoTIFF
```bash
docker-compose exec web-api python << 'EOF'
from climate_embeddings.loaders import load_raster_auto

result = load_raster_auto("data/external/temperature_map.tif")
print(f"‚úì Loaded {len(result.embeddings)} embeddings")
print(f"  Bounds: {result.metadata.get('bounds')}")
EOF
```

#### CSV
```bash
docker-compose exec web-api python << 'EOF'
from climate_embeddings.loaders import load_raster_auto

result = load_raster_auto("data/external/station_data.csv")
print(f"‚úì Loaded {len(result.embeddings)} embeddings")
EOF
```

#### ZIP arch√≠v
```bash
docker-compose exec web-api python << 'EOF'
from climate_embeddings.loaders.raster_pipeline import load_from_zip

results = load_from_zip("data/external/climate_bundle.zip")
print(f"‚úì Loaded {len(results)} files from ZIP")
for r in results:
    print(f"  - {r.source_file}: {len(r.embeddings)} embeddings")
EOF
```

## üß™ Unit testy

### V≈°etky testy
```bash
make test
```

### ≈†pecifick√© test suites
```bash
make test-raster       # Raster loading (NetCDF, GeoTIFF, CSV)
make test-rag          # RAG komponenty (embeddings, index, pipeline)
make test-embeddings   # Qdrant integr√°cia
make test-dagster      # Dagster jobs
make test-api          # Web API endpoints
```

### Test coverage
```bash
make test-coverage     # Generuje HTML report do htmlcov/
```

## üîç Testovanie embeddings

### Text embeddings

```bash
# BGE model (1024-dim)
docker-compose exec web-api python << 'EOF'
from climate_embeddings.embeddings import get_text_embedder

embedder = get_text_embedder("bge-large")
embedding = embedder.encode("temperature data from ERA5")
print(f"‚úì BGE embedding shape: {embedding.shape}")  # (1024,)
EOF

# GTE model (1024-dim)
docker-compose exec web-api python << 'EOF'
from climate_embeddings.embeddings import get_text_embedder

embedder = get_text_embedder("gte-large")
embedding = embedder.encode("precipitation trends")
print(f"‚úì GTE embedding shape: {embedding.shape}")  # (1024,)
EOF

# MiniLM model (384-dim, r√Ωchly)
docker-compose exec web-api python << 'EOF'
from climate_embeddings.embeddings import get_text_embedder

embedder = get_text_embedder("minilm")
embedding = embedder.encode("wind speed data")
print(f"‚úì MiniLM embedding shape: {embedding.shape}")  # (384,)
EOF
```

### Vector index

```bash
docker-compose exec web-api python << 'EOF'
import numpy as np
from climate_embeddings.index import VectorIndex

# Vytvori≈• index
index = VectorIndex(dimension=1024, metric="cosine")

# Prida≈• vektory
vectors = np.random.randn(100, 1024).astype(np.float32)
metadata = [{"id": i, "type": "test"} for i in range(100)]
index.add_batch(vectors, metadata)

# Vyhƒæad√°vanie
query = np.random.randn(1024).astype(np.float32)
results = index.search(query, k=5)

print(f"‚úì Added {len(vectors)} vectors")
print(f"‚úì Found {len(results)} nearest neighbors")
for r in results:
    print(f"  - Score: {r.score:.4f}, Metadata: {r.metadata}")
EOF
```

## ü§ñ RAG Pipeline Testing

### Kompletn√Ω RAG workflow

```bash
docker-compose exec web-api python << 'EOF'
import numpy as np
from climate_embeddings.rag import RAGPipeline
from climate_embeddings.index import VectorIndex

# 1. Vytvori≈• index s d√°tami
index = VectorIndex(dimension=1024, metric="cosine")

# Prida≈• nejak√© vektory s metad√°tami
vectors = np.random.randn(10, 1024).astype(np.float32)
metadata = [
    {"text": "Temperature in Europe increased by 1.5¬∞C since 2000"},
    {"text": "Precipitation patterns changed in Mediterranean region"},
    {"text": "Arctic sea ice extent decreased by 40%"},
    {"text": "CO2 concentrations reached 420 ppm in 2024"},
    {"text": "Heat waves became more frequent in summer"},
    {"text": "Drought conditions persisted in Central Europe"},
    {"text": "Sea level rose by 3mm per year globally"},
    {"text": "Extreme weather events increased in frequency"},
    {"text": "Glaciers in Alps retreated significantly"},
    {"text": "Ocean temperatures reached record highs"}
]
index.add_batch(vectors, metadata)

# 2. Vytvori≈• RAG pipeline
rag = RAGPipeline(
    index=index,
    embedder_name="bge-large",
    llm_model="llama3.2:1b",
    llm_base_url="http://ollama:11434"
)

# 3. Retrieve relevantn√© dokumenty
results = rag.retrieve("What are the temperature trends?", k=3)
print("‚úì Retrieved documents:")
for r in results:
    print(f"  - {r.metadata['text']} (score: {r.score:.4f})")

# 4. RAG query s LLM generovan√≠m
print("\n‚úì Asking RAG system...")
answer = rag.ask("What are the main climate changes observed?", k=5)
print(f"Answer: {answer}")
EOF
```

## üíæ Qdrant Integration Testing

### Basic Qdrant operations

```bash
# Vytvori≈• kolekciu a ulo≈æi≈• embeddings
docker-compose exec web-api python << 'EOF'
import numpy as np
from src.embeddings.database import VectorDatabase
from src.embeddings.generator import EmbeddingGenerator

# Generova≈• text embeddings
generator = EmbeddingGenerator()
texts = [
    "Temperature data from ERA5 reanalysis",
    "Precipitation measurements from weather stations",
    "Wind speed data from climate models",
    "Sea level pressure observations"
]
embeddings = generator.generate_embeddings(texts)

# Ulo≈æi≈• do Qdrant
db = VectorDatabase(collection_name="climate_data")
metadata = [{"text": t, "type": "climate", "idx": i} for i, t in enumerate(texts)]
db.add_embeddings(embeddings, metadata)

print(f"‚úì Stored {len(embeddings)} embeddings in Qdrant")
print(f"  Collection: {db.collection_name}")
print(f"  Dimension: {embeddings.shape[1]}")
EOF

# Semantic search
docker-compose exec web-api python << 'EOF'
from src.embeddings.search import SemanticSearcher
from src.embeddings.database import VectorDatabase

db = VectorDatabase(collection_name="climate_data")
searcher = SemanticSearcher(database=db)

results = searcher.search("temperature and climate", k=3)
print("‚úì Search results:")
for r in results:
    print(f"  - {r['metadata']['text']}")
    print(f"    Similarity: {r['similarity']:.4f}")
EOF
```

## üåê API Endpoint Testing

### List sources
```bash
curl http://localhost:8000/sources | jq
```

### List jobs
```bash
curl http://localhost:8000/jobs | jq
```

### Trigger ETL job
```bash
make trigger-etl

# Alebo manu√°lne
curl -X POST http://localhost:8000/jobs/dynamic_source_etl_job/run \
  -H "Content-Type: application/json" \
  -d '{}'
```

### RAG query endpoint
```bash
curl -X POST http://localhost:8000/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What are the temperature trends in Europe?",
    "top_k": 5
  }' | jq
```

## ‚öôÔ∏è Dagster Testing

### Spustenie Dagster UI

```bash
make dagit
# Otvor http://localhost:3000
```

### Verifik√°cia Dagster workspace

```bash
docker-compose exec dagit python << 'EOF'
from dagster import DagsterInstance
from dagster_project.repository import climate_repository

repo = climate_repository()
print(f"‚úì Repository: {repo.name}")
print("\nAvailable jobs:")
for job in repo.get_all_jobs():
    print(f"  - {job.name}")
    
print("\nSchedules:")
for schedule in repo.get_all_schedules():
    print(f"  - {schedule.name}: {schedule.cron_schedule}")
EOF
```

### Spustenie job manu√°lne

```bash
# Cez API
make trigger-etl
make trigger-embeddings

# Cez CLI (v containeri)
docker-compose exec dagster-daemon dagster job execute \
  -m dagster_project.repository \
  -j dynamic_source_etl_job
```

## üìä Monitoring & Logs

### Docker logs
```bash
make dagster-logs           # Dagster slu≈æby
docker-compose logs -f web-api
docker-compose logs -f qdrant
docker-compose logs -f ollama
```

### Dagster compute logs
```bash
# V Dagster UI: Runs ‚Üí vyberte run ‚Üí Compute Logs
# Alebo na serveri:
ls -la .dagster_home/storage/
```

### API logs
```bash
docker-compose logs -f web-api | grep -i error
```

## üîß Troubleshooting

### Services not starting
```bash
# Re≈°tartova≈• slu≈æby
docker-compose restart

# Rebuildi≈• image
make docker-build
docker-compose up -d --build
```

### Import errors
```bash
# Skontrolova≈• Python path
docker-compose exec web-api python -c "import sys; print('\\n'.join(sys.path))"

# Skontrolova≈• in≈°talovan√© bal√≠ƒçky
docker-compose exec web-api pip list | grep -i climate
```

### Qdrant connection issues
```bash
# Skontrolova≈• Qdrant health
curl http://localhost:6333/health

# Zoznam kolekci√≠
curl http://localhost:6333/collections | jq
```

### Ollama model issues
```bash
# Skontrolova≈• dostupn√© modely
curl http://localhost:11434/api/tags | jq

# Stiahnu≈• model
docker-compose exec ollama ollama pull llama3.2:1b
```

## ‚úÖ Checklist pre deployment testing

- [ ] Docker slu≈æby be≈æia (`docker-compose ps`)
- [ ] Qdrant zdrav√Ω (`make check-qdrant`)
- [ ] Ollama zdrav√Ω (`make check-ollama`)
- [ ] API zdrav√© (`make api-health`)
- [ ] Importy funguj√∫ (`make test-formats` kroky 1-2)
- [ ] NetCDF loading funguje
- [ ] GeoTIFF loading funguje
- [ ] CSV loading funguje
- [ ] ZIP loading funguje
- [ ] Text embeddings funguj√∫ (BGE, GTE)
- [ ] Vector index funguje
- [ ] RAG pipeline funguje
- [ ] Qdrant ukladanie funguje
- [ ] Semantic search funguje
- [ ] API endpoints funguj√∫
- [ ] Dagster jobs sa daj√∫ spusti≈•
- [ ] Unit testy prech√°dzaj√∫ (`make test`)

## üìù Quick Commands Reference

```bash
# Setup
make docker-compose-up      # Spust√≠ v≈°etky slu≈æby
make verify-services        # Over√≠ zdravie slu≈æieb

# Testing
make test-formats          # Test v≈°etk√Ωch form√°tov
make test-raster           # Test raster loading
make test-rag              # Test RAG komponenty
make test-embeddings       # Test Qdrant
make test-all              # V≈°etky testy

# Services
make dagit                 # Dagster UI (port 3000)
make api                   # API service (port 8000)
make dagster-logs          # Zobrazi≈• logy

# Triggers
make trigger-etl           # Spusti≈• ETL job
make trigger-embeddings    # Spusti≈• embedding job

# Checks
make check-qdrant          # Qdrant status
make check-ollama          # Ollama status
make api-health            # API health
make list-sources          # Zoznam zdrojov

# Cleanup
make docker-compose-down   # Zastavi≈• slu≈æby
make clean                 # Vyƒçisti≈• cache
```

## üéØ Production Deployment Checklist

1. **Environment variables** - skontrolova≈• `.env`
2. **Data volumes** - namapova≈• `/data` persistent storage
3. **Qdrant persistence** - volume pre `/qdrant/storage`
4. **Ollama models** - predstiahnu≈• potrebn√© modely
5. **Resource limits** - nastavi≈• v docker-compose.yml
6. **Logging** - konfigurova≈• log aggregation
7. **Monitoring** - Prometheus/Grafana pre metrics
8. **Backups** - automatick√Ω backup Qdrant collections

---

**Pozn√°mka:** V≈°etky testy predpokladaj√∫ ≈æe slu≈æby be≈æia cez `docker-compose up -d`.
