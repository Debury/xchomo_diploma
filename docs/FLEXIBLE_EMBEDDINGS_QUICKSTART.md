# Flexible Embeddings - Quick Start Guide

## ÄŒo je novÃ©?

Embedding systÃ©m bol kompletne prepracovanÃ½:
- âŒ **ODSTRÃNENÃ‰:** Hardcoded Å¡ablÃ³ny a premennÃ©
- âœ… **PRIDANÃ‰:** AutomatickÃ¡ detekcia akÃ½chkoÄ¾vek dÃ¡t
- âœ… **PRIDANÃ‰:** Sample values pre RAG
- âœ… **PRIDANÃ‰:** Tri Ãºrovne verbosity

## RÃ½chly Å¡tart

### 1. Spracovanie datasetu

```python
from src.embeddings import EmbeddingPipeline

# InicializÃ¡cia pipeline
pipeline = EmbeddingPipeline()

# Spracovanie AKÃ‰HOKOÄ½VEK datasetu
result = pipeline.process_dataset("data/processed/your_data.nc")

print(f"VytvorenÃ½ch embeddings: {result['num_embeddings']}")
```

### 2. VyhÄ¾adÃ¡vanie (RAG-ready)

```python
from src.embeddings import SemanticSearcher

# InicializÃ¡cia vyhÄ¾adÃ¡vaÄa
searcher = SemanticSearcher()

# VyhÄ¾adÃ¡vanie v prirodzenom jazyku
results = searcher.search(
    "teplota okolo 15 stupÅˆov",
    k=5
)

# VÃ½sledky obsahujÃº:
for result in results:
    print(result['metadata']['variable'])      # NÃ¡zov premennej
    print(result['metadata']['stat_mean'])     # Priemer
    print(result['metadata']['sample_values']) # Sample hodnoty!
    print(result['document'])                  # PlnÃ½ text
```

### 3. VlastnÃ¡ extrakcia metadata

```python
from src.embeddings import MetadataExtractor
import xarray as xr

# NaÄÃ­tanie datasetu
ds = xr.open_dataset("data.nc")

# Extrakcia metadata
extractor = MetadataExtractor()
metadata_list = extractor.extract_from_dataset(
    data=ds,
    file_path="data.nc",
    dataset_id="my_dataset"
)

# Metadata obsahuje VÅ ETKO:
meta = metadata_list[0]
print(meta['variable'])           # NÃ¡zov premennej
print(meta['stat_mean'])          # Å tatistiky
print(meta['sample_values'])      # Sample hodnoty
print(meta['spatial_extent'])     # PriestorovÃ© info
print(meta['temporal_extent'])    # ÄŒasovÃ© info
```

### 4. Generovanie textu

```python
from src.embeddings import TextGenerator

# RÃ´zne Ãºrovne verbosity
for verbosity in ['low', 'medium', 'high']:
    text_gen = TextGenerator(config={'verbosity': verbosity})
    text = text_gen.generate_document(metadata)
    print(f"\n{verbosity.upper()}:\n{text}")
```

## PrÃ­klad vÃ½stupu

### Low verbosity:
```
Dataset 'era5' contains variable 't2m' (2m temperature) measured in K | 
Statistics: mean=281.26K, range=[221.00, 317.40]K | 
Coordinates: latitude range [15.00, 75.00]Â°, longitude range [200.00, 330.00]Â° | 
Sample values (K): [241.20, 280.60, 292.79, ..., 293.40, 295.69]
```

### Medium verbosity (odporÃºÄanÃ© pre RAG):
```
Dataset 'era5' contains variable 't2m' (2m temperature) measured in K | 
Statistics: mean=281.26K, range=[221.00, 317.40]K, std=16.32K, 3869000 data points | 
Coordinates: latitude range [15.00, 75.00]Â°, longitude range [200.00, 330.00]Â°, 
time period from 2013-01-01 to 2014-12-31, 6h frequency, dimensions: time=2920, lat=25, lon=53 | 
Sample values (K): [241.20, 280.60, 292.79, ..., 293.40, 295.69]
```

### High verbosity:
```
Dataset 'era5' contains variable 't2m' (2m temperature) measured in K | 
Statistics: count=3869000, max=317.40K, mean=281.26K, median=285.20K, min=221.00K, std=16.32K | 
Coordinates: latitude range [15.00, 75.00]Â°, longitude range [200.00, 330.00]Â°, 
time period from 2013-01-01 to 2014-12-31, 6h frequency, dimensions: time=2920, lat=25, lon=53 | 
Sample values (K): [241.20, 280.60, 292.79, ..., 293.40, 295.69] | 
Additional info: Variable attributes: precision=2, GRIB_id=11, GRIB_name=TMP | 
title: ERA5 Reanalysis | references: https://...
```

## KonfigurÃ¡cia

Upravte `config/pipeline_config.yaml`:

```yaml
embeddings:
  statistics:
    - "mean"
    - "min"
    - "max"
    - "std"
    - "median"
    # Pridajte Ä¾ubovoÄ¾nÃ© ÄalÅ¡ie...
  
  text_generation:
    include_sample_values: true   # DÃ´leÅ¾itÃ© pre RAG!
    include_statistics: true
    include_coordinates: true
    include_attributes: true
    verbosity: "medium"           # low/medium/high
```

## RAG Integration

```python
from src.embeddings import SemanticSearcher

# Setup
searcher = SemanticSearcher()

# VyhÄ¾adÃ¡vanie pre RAG
results = searcher.search("Äo je priemernÃ¡ teplota?", k=3)

# Pripravte kontext pre LLM
context = ""
for r in results:
    meta = r['metadata']
    context += f"""
    PremennÃ¡: {meta['variable']}
    Priemer: {meta['stat_mean']}
    Rozsah: [{meta['stat_min']}, {meta['stat_max']}]
    Vzorky: {meta['sample_values']}
    
    {r['document']}
    ---
    """

# PoÅ¡lite do LLM
# response = llm.generate(f"Context: {context}\n\nQuestion: {query}")
```

## Testovanie

```bash
# Spustite test
python scripts/test_flexible_embeddings.py
```

Test overÃ­:
- âœ… AutomatickÃº detekciu dimenziÃ­
- âœ… Extrakciu vÅ¡etkÃ½ch premennÃ½ch
- âœ… Generovanie textu vo vÅ¡etkÃ½ch verbosity levels
- âœ… Vytvorenie embeddings
- âœ… Semantic search s RAG

## PodporovanÃ© formÃ¡ty

- âœ… NetCDF (.nc)
- âœ… Parquet (.parquet)
- âœ… CSV (.csv)

SystÃ©m automaticky detekuje:
- ğŸ“ Latitude/Longitude (lat, lon, latitude, longitude, y, x)
- â° Time (time, valid_time, date, datetime)
- ğŸ“ Levels (level, pressure, height, vertical)
- ğŸ“Š VÅ¡etky numerickÃ© premennÃ©

## Rozdiel oproti starej verzii

| StarÃ¡ verzia | NovÃ¡ verzia |
|-------------|-------------|
| Hardcoded template | DynamickÃ© generovanie |
| Å pecifickÃ© premennÃ© (t2m, tp) | AkÃ©koÄ¾vek premennÃ© |
| Bez sample values | âœ… Sample values pre RAG |
| Jedna verbosity | âœ… 3 Ãºrovne verbosity |
| FixnÃ© dimenzie | âœ… Auto-detekcia dimenziÃ­ |

## FAQ

**Q: Funguje to s mojimi dÃ¡tami?**  
A: Ãno! Ak mÃ¡Å¡ NetCDF, CSV alebo Parquet s ÄÃ­selnÃ½mi dÃ¡tami, funguje to.

**Q: MusÃ­m meniÅ¥ konfigurÃ¡ciu pre novÃ½ dataset?**  
A: Nie! SystÃ©m automaticky deteguje vÅ¡etko.

**Q: Ako pridÃ¡m novÃ© Å¡tatistiky?**  
A: Pridaj ich do `config/pipeline_config.yaml` pod `embeddings.statistics`.

**Q: PreÄo sÃº tam sample values?**  
A: Pre RAG! LLM model vidÃ­ skutoÄnÃ© hodnoty a mÃ´Å¾e lepÅ¡ie odpovedaÅ¥ na otÃ¡zky.

**Q: KtorÃº verbosity pouÅ¾iÅ¥?**  
A: `medium` - najlepÅ¡Ã­ pomer informÃ¡cie/dÄºÅ¾ka pre embeddings.

## ÄalÅ¡ie prÃ­klady

### Batch processing:
```python
pipeline = EmbeddingPipeline()

# Spracuj celÃ½ adresÃ¡r
result = pipeline.process_directory(
    "data/processed",
    pattern="*.nc",
    recursive=True
)

print(f"SpracovanÃ½ch sÃºborov: {result['num_files']}")
print(f"VytvorenÃ½ch embeddings: {result['num_embeddings']}")
```

### Custom metadata config:
```python
config = {
    'statistics': ['mean', 'min', 'max', 'percentile_95'],
    'verbosity': 'high'
}

extractor = MetadataExtractor(config=config)
text_gen = TextGenerator(config=config)
```

## ZÃ¡ver

SystÃ©m je **production-ready** a funguje s akÃ½mikoÄ¾vek meteorologickÃ½mi dÃ¡tami!

NevÃ¡haj a testuj na svojich datasetoch! ğŸš€
